<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel><title>Зайчатки разума</title><link>https://shumiloff.ru/index.html</link>
<description>Записная книжка айтишника</description><language>en</language>
<lastBuildDate>Sun, 16 Jun 2024 02:59:48 +0500</lastBuildDate>
<pubDate>Sun, 16 Jun 2024 02:59:48 +0500</pubDate>
<atom:link href="https://shumiloff.ru/feed.rss" rel="self" type="application/rss+xml" />
<item><title>
Про идентификацию одинаковых подключаемых USB устройств в Linux
</title><description><![CDATA[

<p><img alt="" src="images/linux_usb_identification.jpg" /></p>

<p>Возникла у меня идея - подключить к одному перепрошитому под linux TV Box&#39;у три 3D принтера, чтобы он один управлял с помощью трёх сессий octoprint ими всеми, ещё и транслировал изображение с камер. Возник ряд сложностей - как идентифицировать конкретное устройство, чтобы назначить на него определённый octoprint?</p>

<p>Дело в том, что Linux создаёт tty устройства в /dev/ в порядке их подключения, т.е. первым подключенным/включенным будет /dev/ttyUSB0, вторым - /dev/ttyUSB1 и так далее. Включением/выключением устройств управляет система умного дома. Закончилась печать - принтер выключился. Нужно что-то напечатать - включился через умную розетку. Ещё и энергопотребление попутно логируется. И в различные моменты времени один и тот же принтер может быть подключен и к /dev/ttyUSB0 и к /dev/USB2, например, если /dev/ttyUSB0 и /dev/ttyUSB1 уже заняты.</p>

<hr />
<p>Теги: <a href='tag_3d-printing.html'>3d-printing</a>, <a href='tag_linux.html'>linux</a></p>
]]></description><link>https://shumiloff.ru/pro-identifikaciyu-odinakovyx-podklyuchaemyx-usb-ustrojstv-v-linux.html</link>
<guid>https://shumiloff.ru/./pro-identifikaciyu-odinakovyx-podklyuchaemyx-usb-ustrojstv-v-linux.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sun, 16 Jun 2024 02:57:32 +0500</pubDate></item>
<item><title>
Про недооценённые утилиты - dufs
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/dufs.png" style="height:527px; width:800px" /></p>

<p>&nbsp; Иногда бывает такое, что тебе для конкретной задачи не хватает определённого инструмента. Ты его ищешь, ищешь, ищешь, потом уже устаёшь и перестаёшь искать, а спустя несколько лет случайно натыкаешься на него и понимаешь - вот оно! Именно то, что нужно и именно в том виде, который и требовалось. И тогда радость от обретения куда больше, чем если бы ты нашёл его сразу.</p>

<p>&nbsp; Из таких случаев мне навскидку вспомнилось несколько - <a href="https://shumiloff.ru/sam-sebe-xosting-ili-o-nedoocenyonnyx-utilitax.html">MySecureShell</a> и <a href="https://shumiloff.ru/prostaya-i-bystraya-nastrojka-mesh-vpn-s-pomoshhyu-tinc.html">tinc</a>, о которых я уже писал , lnav, о котором я возможно ещё напишу, mosh и вот, недавно - <a href="https://github.com/sigoden/dufs">dufs</a>.</p>

<hr />
<p>Теги: <a href='tag_shell.html'>shell</a>, <a href='tag_instruments.html'>instruments</a></p>
]]></description><link>https://shumiloff.ru/pro-nedoocenyonnye-utility---dufs.html</link>
<guid>https://shumiloff.ru/./pro-nedoocenyonnye-utility---dufs.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sun, 21 Jan 2024 23:31:04 +0500</pubDate></item>
<item><title>
</title><description><![CDATA[
]]></description><link>https://shumiloff.ru/test.html</link>
<guid>https://shumiloff.ru/./test.html</guid>
<dc:creator></dc:creator>
<pubDate>Sun, 21 Jan 2024 22:39:37 +0500</pubDate></item>
<item><title>
Статья на cnews
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/rukovoditel.jpg" /></p>

<p>&nbsp; Почти три года назад у меня родился замечательный сын. Потом я получил должность технического директора в нашей компании, потом случилась ипотека и переезд, который растянулся практически на полгода и, как и любой переезд, сопровождался миллионом дел и забот - одним словом, количество свободного времени устремилось не то что к нулю, а в пространство отрицательных чисел. Это можно заметить и по количеству статей, которые появлялись на моём блоге ранее по нескольку штук в неделю, а теперь раз в год. Это печально, хотелось бы больше времени посвящать и блогу, но пока к сожалению, это нереально.</p>

<p><br />
&nbsp; А между тем произошло множество событий в моей жизни, о которых я почти забыл, но хотелось бы помнить. Например, я заслужил звание &quot;Руководитель года&quot;, а 31го января 2023го года вышла <a href="https://cnews.ru/link/a19175" target="_blank">моя статья на cnews</a>, которую я хотел бы привести и тут.</p>

<hr />
<p>Теги: <a href='tag_work.html'>work</a></p>
]]></description><link>https://shumiloff.ru/statya-na-cnews.html</link>
<guid>https://shumiloff.ru/./statya-na-cnews.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sat, 27 May 2023 14:12:18 +0500</pubDate></item>
<item><title>
Скрипт для поиска распухших логов докера
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/docker-logs.png" /></p>

<p>&nbsp; В который раз уже сталкиваюсь с этой ситуацией. Субботнее утро, хочется немного попрокрастинировать (что при загрузке 60-70 часов в неделю просто жизненно необходимо) и тут начинают сыпаться алерты - заканчивается место на диске одного из рабочих серверов. Как всегда запускаем в <a href="https://sourceforge.net/projects/tmux.mirror/" target="_blank">tmux</a> команду sudo ncdu -x / и ждём. Пришлось ждать минут 40, так как количество файлов на хосте действительно велико, в основном за счёт кешей npm и node modules. И опять ожидаемо обнаружилось, что несколько сотен гигабайт съели логи новых докер контейнеров, запущенных разработчиками на хосте разработки.</p>

<hr />
<p>Теги: <a href='tag_docker.html'>docker</a>, <a href='tag_админское.html'>админское</a></p>
]]></description><link>https://shumiloff.ru/skript-dlya-poiska-raspuxshix-logov-dokera.html</link>
<guid>https://shumiloff.ru/./skript-dlya-poiska-raspuxshix-logov-dokera.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sat, 27 May 2023 13:44:08 +0500</pubDate></item>
<item><title>
Про выигрыш и победу
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/yacht.jpg" /></p>

<p>&nbsp; Не так давно у нас был корпоратив. Основным гвоздём программы был яхтинг, а точнее даже &quot;парусная регата&quot;. Происходило данное мероприятие недалеко от мыса &quot;Стрелка&quot; и КамГЭС, задача была достаточно простая - подойти к бую, на парусном вооружении, от него пройти до буя на противоположном берегу у мыса &quot;Стрелка&quot;, обогнуть его и вернуться обратно за минимально возможное время. Всех участвующих разделили на 14 команд и распределили путём жребьёвки по семи яхтам в два захода. Я в числе прочих своих сослуживцев оказался на небольшой 27-ми футовой яхте &quot;Family&quot;, познакомился со шкипером и его дочерью (крайне душевные ребята), сообщил, что в яхтенных походах уже бывал и мною могут располагать - отличаю гротофал от стаксель шкота, кранец от кракена, принёс две пары нормальных яхтенных перчаток и вообще, полезный малый.</p>

<hr />
<p>Теги: <a href='tag_жизненное.html'>жизненное</a></p>
]]></description><link>https://shumiloff.ru/pro-vyigrysh-i-pobedu.html</link>
<guid>https://shumiloff.ru/./pro-vyigrysh-i-pobedu.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Wed, 31 Aug 2022 00:17:04 +0500</pubDate></item>
<item><title>
Про два года и сискуль
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/andrey.png" /></p>

<p>&nbsp; Много раз слышал о том, что чужие дети растут быстро, а свои ещё быстрее. И это похоже одна из тех вещей, о которых можно сто раз услышать, знать об этом, но понять - только прочувствовав на себе. Все эти &quot;вот появятся свои, тогда поймёшь&quot; - как ни удивительно, в большинстве своём правда.</p>

<hr />
<p>Теги: <a href='tag_жизненное.html'>жизненное</a>, <a href='tag_детёныш.html'>детёныш</a></p>
]]></description><link>https://shumiloff.ru/pro-dva-goda-i-siskul.html</link>
<guid>https://shumiloff.ru/./pro-dva-goda-i-siskul.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Fri, 03 Jun 2022 00:13:34 +0500</pubDate></item>
<item><title>
Работа с GRE туннелями или история одного велосипеда
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/grescheme.png" style="height:321px; width:630px" /></p>

<p>&nbsp; Почему периодически айтишники делают велосипеды? Ответ простой - потому что на своём велосипеде ездить удобнее. Ну кто хотя бы раз не написал свой модуль логирования для какого-нибудь языка?</p>

<h4>Постановка задачи</h4>

<p>&nbsp; На объекте есть какие-то подсети, в которых живут инженерные системы. И есть ряд хостов в датацентрах, с которых необходимо получить доступ до этих самых инженерных объектов. В качестве сетевого оборудования на объекте часто либо Cisco, либо Microtic, либо некая линуксовая машина (но на этот случай есть другие, более приятные и удобные для нас инструменты). Исторически так сложилось, что штатным для нас методом обеспечения связности являются <a href="https://ru.wikipedia.org/wiki/GRE_(%D0%BF%D1%80%D0%BE%D1%82%D0%BE%D0%BA%D0%BE%D0%BB">GRE туннели</a>) с поднятием маршрутизации в нужные подсети через конечные точки туннеля с последующим закрытием доступа файрволом по белым адресам с обеих сторон. Вопросы шифрования туннелей пока не поднимаем, это возможно будет темой для отдельной статьи.</p>

<hr />
<p>Теги: <a href='tag_linux.html'>linux</a>, <a href='tag_networking.html'>networking</a>, <a href='tag_shell.html'>shell</a>, <a href='tag_админское.html'>админское</a></p>
]]></description><link>https://shumiloff.ru/rabota-s-gre-tunnelyami-ili-istoriya-odnogo-velosipeda.html</link>
<guid>https://shumiloff.ru/./rabota-s-gre-tunnelyami-ili-istoriya-odnogo-velosipeda.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Wed, 01 Jun 2022 13:48:24 +0500</pubDate></item>
<item><title>
Как понять, где слонику стало тяжело
</title><description><![CDATA[

<p><img alt="" src="images/sqlload1.png" /></p>

<p>&nbsp; На работе столкнулся с проблемой - судя по мониторингу резко начинает расти load average, причём увеличивается количество форков postgres и суммарная нагрузка на CPU, которую потребляет postgres начинает зашкаливать...</p>

<hr />
<p>Теги: <a href='tag_shell.html'>shell</a>, <a href='tag_админское.html'>админское</a></p>
]]></description><link>https://shumiloff.ru/kak-ponyat-gde-sloniku-stalo-tyazhelo.html</link>
<guid>https://shumiloff.ru/./kak-ponyat-gde-sloniku-stalo-tyazhelo.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Fri, 29 Oct 2021 23:25:50 +0500</pubDate></item>
<item><title>
Небольшая проблема с софтовым рейд массивом
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/raid.png" /></p>

<p>&nbsp; Преамбула банальна: Жил/был сервер с софтовым зеркалом. И вот, начинаются странные проблемы. То я фаил сохранить не могу и vim наглухо зависает, то считать что-то не могу. Полез первым делом в dmesg, а там красота нечеловеческая на два экрана, приведённая на скриншоте в заголовке.</p>

<p>&nbsp; Прогнал smartctl, написал в службу техподдержки Selectel - мол, диску похоже пришёл северный зверь из семейства куньих. Можете ли заменить диск? И вот техподдержка селектела реально порадовала - мол, да-да, конечно, ошибок смарта не видим, но заменим обязательно, напишите, когда вам удобно и т.п.. Если честно, я такой оперативности и учтивости после гуглооблака и AWS прямо не ожидал. Уточнил детали - мол, поддерживает ли диск hotswap, можно работают ли они круглосуточно и т.п..</p>

<hr />
<p>Теги: <a href='tag_админское.html'>админское</a>, <a href='tag_linux.html'>linux</a></p>
]]></description><link>https://shumiloff.ru/nebolshaya-problema-s-softovym-rejd-massivom.html</link>
<guid>https://shumiloff.ru/./nebolshaya-problema-s-softovym-rejd-massivom.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Mon, 18 Oct 2021 22:10:11 +0500</pubDate></item>
</channel></rss>
