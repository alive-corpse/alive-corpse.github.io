<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel><title>Зайчатки разума</title><link>https://shumiloff.ru/index.html</link>
<description>Записная книжка айтишника</description><language>en</language>
<lastBuildDate>Sat, 27 May 2023 13:44:50 +0500</lastBuildDate>
<pubDate>Sat, 27 May 2023 13:44:50 +0500</pubDate>
<atom:link href="https://shumiloff.ru/feed.rss" rel="self" type="application/rss+xml" />
<item><title>
Скрипт для поиска распухших логов докера
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/docker-logs.png" /></p>

<p>&nbsp; В который раз уже сталкиваюсь с этой ситуацией. Субботнее утро, хочется немного попрокрастинировать (что при загрузке 60-70 часов в неделю просто жизненно необходимо) и тут начинают сыпаться алерты - заканчивается место на диске одного из рабочих серверов. Как всегда запускаем в <a href="https://sourceforge.net/projects/tmux.mirror/" target="_blank">tmux</a> команду sudo ncdu -x / и ждём. Пришлось ждать минут 40, так как количество файлов на хосте действительно велико, в основном за счёт кешей npm и node modules. И опять ожидаемо обнаружилось, что несколько сотен гигабайт съели логи новых докер контейнеров, запущенных разработчиками на хосте разработки.</p>

<hr />
<p>Теги: <a href='tag_docker.html'>docker</a>, <a href='tag_админское.html'>админское</a></p>
]]></description><link>https://shumiloff.ru/skript-dlya-poiska-raspuxshix-logov-dokera.html</link>
<guid>https://shumiloff.ru/./skript-dlya-poiska-raspuxshix-logov-dokera.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sat, 27 May 2023 13:44:08 +0500</pubDate></item>
<item><title>
Про выигрыш и победу
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/yacht.jpg" /></p>

<p>&nbsp; Не так давно у нас был корпоратив. Основным гвоздём программы был яхтинг, а точнее даже &quot;парусная регата&quot;. Происходило данное мероприятие недалеко от мыса &quot;Стрелка&quot; и КамГЭС, задача была достаточно простая - подойти к бую, на парусном вооружении, от него пройти до буя на противоположном берегу у мыса &quot;Стрелка&quot;, обогнуть его и вернуться обратно за минимально возможное время. Всех участвующих разделили на 14 команд и распределили путём жребьёвки по семи яхтам в два захода. Я в числе прочих своих сослуживцев оказался на небольшой 27-ми футовой яхте &quot;Family&quot;, познакомился со шкипером и его дочерью (крайне душевные ребята), сообщил, что в яхтенных походах уже бывал и мною могут располагать - отличаю гротофал от стаксель шкота, кранец от кракена, принёс две пары нормальных яхтенных перчаток и вообще, полезный малый.</p>

<hr />
<p>Теги: <a href='tag_жизненное.html'>жизненное</a></p>
]]></description><link>https://shumiloff.ru/pro-vyigrysh-i-pobedu.html</link>
<guid>https://shumiloff.ru/./pro-vyigrysh-i-pobedu.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Wed, 31 Aug 2022 00:17:04 +0500</pubDate></item>
<item><title>
Про два года и сискуль
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/andrey.png" /></p>

<p>&nbsp; Много раз слышал о том, что чужие дети растут быстро, а свои ещё быстрее. И это похоже одна из тех вещей, о которых можно сто раз услышать, знать об этом, но понять - только прочувствовав на себе. Все эти &quot;вот появятся свои, тогда поймёшь&quot; - как ни удивительно, в большинстве своём правда.</p>

<hr />
<p>Теги: <a href='tag_жизненное.html'>жизненное</a>, <a href='tag_детёныш.html'>детёныш</a></p>
]]></description><link>https://shumiloff.ru/pro-dva-goda-i-siskul.html</link>
<guid>https://shumiloff.ru/./pro-dva-goda-i-siskul.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Fri, 03 Jun 2022 00:13:34 +0500</pubDate></item>
<item><title>
Работа с GRE туннелями или история одного велосипеда
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/grescheme.png" style="height:321px; width:630px" /></p>

<p>&nbsp; Почему периодически айтишники делают велосипеды? Ответ простой - потому что на своём велосипеде ездить удобнее. Ну кто хотя бы раз не написал свой модуль логирования для какого-нибудь языка?</p>

<h4>Постановка задачи</h4>

<p>&nbsp; На объекте есть какие-то подсети, в которых живут инженерные системы. И есть ряд хостов в датацентрах, с которых необходимо получить доступ до этих самых инженерных объектов. В качестве сетевого оборудования на объекте часто либо Cisco, либо Microtic, либо некая линуксовая машина (но на этот случай есть другие, более приятные и удобные для нас инструменты). Исторически так сложилось, что штатным для нас методом обеспечения связности являются <a href="https://ru.wikipedia.org/wiki/GRE_(%D0%BF%D1%80%D0%BE%D1%82%D0%BE%D0%BA%D0%BE%D0%BB">GRE туннели</a>) с поднятием маршрутизации в нужные подсети через конечные точки туннеля с последующим закрытием доступа файрволом по белым адресам с обеих сторон. Вопросы шифрования туннелей пока не поднимаем, это возможно будет темой для отдельной статьи.</p>

<hr />
<p>Теги: <a href='tag_linux.html'>linux</a>, <a href='tag_networking.html'>networking</a>, <a href='tag_shell.html'>shell</a>, <a href='tag_админское.html'>админское</a></p>
]]></description><link>https://shumiloff.ru/rabota-s-gre-tunnelyami-ili-istoriya-odnogo-velosipeda.html</link>
<guid>https://shumiloff.ru/./rabota-s-gre-tunnelyami-ili-istoriya-odnogo-velosipeda.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Wed, 01 Jun 2022 13:48:24 +0500</pubDate></item>
<item><title>
Как понять, где слонику стало тяжело
</title><description><![CDATA[

<p><img alt="" src="images/sqlload1.png" /></p>

<p>&nbsp; На работе столкнулся с проблемой - судя по мониторингу резко начинает расти load average, причём увеличивается количество форков postgres и суммарная нагрузка на CPU, которую потребляет postgres начинает зашкаливать...</p>

<hr />
<p>Теги: <a href='tag_shell.html'>shell</a>, <a href='tag_админское.html'>админское</a></p>
]]></description><link>https://shumiloff.ru/kak-ponyat-gde-sloniku-stalo-tyazhelo.html</link>
<guid>https://shumiloff.ru/./kak-ponyat-gde-sloniku-stalo-tyazhelo.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Fri, 29 Oct 2021 23:25:50 +0500</pubDate></item>
<item><title>
Небольшая проблема с софтовым рейд массивом
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/raid.png" /></p>

<p>&nbsp; Преамбула банальна: Жил/был сервер с софтовым зеркалом. И вот, начинаются странные проблемы. То я фаил сохранить не могу и vim наглухо зависает, то считать что-то не могу. Полез первым делом в dmesg, а там красота нечеловеческая на два экрана, приведённая на скриншоте в заголовке.</p>

<p>&nbsp; Прогнал smartctl, написал в службу техподдержки Selectel - мол, диску похоже пришёл северный зверь из семейства куньих. Можете ли заменить диск? И вот техподдержка селектела реально порадовала - мол, да-да, конечно, ошибок смарта не видим, но заменим обязательно, напишите, когда вам удобно и т.п.. Если честно, я такой оперативности и учтивости после гуглооблака и AWS прямо не ожидал. Уточнил детали - мол, поддерживает ли диск hotswap, можно работают ли они круглосуточно и т.п..</p>

<hr />
<p>Теги: <a href='tag_админское.html'>админское</a>, <a href='tag_linux.html'>linux</a></p>
]]></description><link>https://shumiloff.ru/nebolshaya-problema-s-softovym-rejd-massivom.html</link>
<guid>https://shumiloff.ru/./nebolshaya-problema-s-softovym-rejd-massivom.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Mon, 18 Oct 2021 22:10:11 +0500</pubDate></item>
<item><title>
Про переход будущего в настоящее и деградацию человеков
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/leaf/leaf-front.jpeg" /></p>

<p>&nbsp; Как это уже не раз бывало, Костя Шагинян (привет тебе огромный) из 100+8 выложил пост, на который я начал отвечать и в процессе понял, что мой ответ лучше будет оформить в виде отдельного поста.</p>

<p>&nbsp; Костя писал про электромобили, противопоставляя их ДВС, а у меня как раз есть что рассказать по этому поводу.</p>

<hr/>
<p>Теги: <a href='tag_auto.html'>auto</a></p>
]]></description><link>https://shumiloff.ru/pro-perexod-budushhego-v-nastoyashhee-i-degradaciyu-chelovekov.html</link>
<guid>https://shumiloff.ru/./pro-perexod-budushhego-v-nastoyashhee-i-degradaciyu-chelovekov.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Wed, 22 Sep 2021 21:44:37 +0500</pubDate></item>
<item><title>
История одного факапа
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="images/sudo-rm-rf.gif" /></p>

<p>&nbsp; Послушал тут предпоследний 769-й выпуск подкаста Radio-T, в котором обсуждение технических ошибок напомнило мне примерно аналогичный случай из моей практики.</p>

<p>&nbsp; Давным давно, когда я ещё работал в xsolla, которая ещё в то время носила название 2pay, у нас по большому счёту был всего один основной сервер в Московском датацентре, на котором крутился весь мир. Передо мной стояла задача написать скрипт, который зачищает старые логи, временные файлы и прочее, что может занимать много места.</p>

<hr />
<p>Теги: <a href='tag_shell.html'>shell</a>, <a href='tag_админское.html'>админское</a>, <a href='tag_жизненное.html'>жизненное</a></p>
]]></description><link>https://shumiloff.ru/istoriya-odnogo-fakapa.html</link>
<guid>https://shumiloff.ru/./istoriya-odnogo-fakapa.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sun, 05 Sep 2021 18:14:18 +0500</pubDate></item>
<item><title>
</title><description><![CDATA[
]]></description><link>https://shumiloff.ru/test.html</link>
<guid>https://shumiloff.ru/./test.html</guid>
<dc:creator></dc:creator>
<pubDate>Sun, 05 Sep 2021 18:03:58 +0500</pubDate></item>
<item><title>
Про профдеформацию
</title><description><![CDATA[

<p style="text-align:center"><img alt="" src="/images/zags.png" /></p>

<p>&nbsp; Вчера мучался с настройкой сетевой связности между ЦОДом одной компании и подсетью телеком оператора на объекте. Бриджи, туннели, роутинг и вот это всё. Сегодня под утро уже приснился сон, перед тем, как жена разбудила. Решил поделиться с вами. :)</p>

<p>&nbsp; Приходим мы с женой в ЗАГС регистрировать ребёнка, а нам тётка и говорит - выдадим вам тридцатую подсеть, будет у вас четыре айпишника - адрес сети, где ребёнок с мамой прописан, широковещательный и два айпишника как раз остаются маме и ребёнку.</p>

<p>Я: - Эй, а я? А как же я?<br />
Тётка: - А что ты? Много ты участия в этом процессе принимал? Она его вынашивала, рожала, ночами не спит. Может вам 29ю подсеть выделить? Куда ещё 3 адреса денете? Я не верю, что вы четверо детей планируете, а с адресами у нас и так дефицит. Ничего, за натом посидишь, сам себе там подсеть выдели серую, найдёшь оборудование.</p>

<p>На этом моменте я проснулся.</p>

<p>Теги: <a href='tag_fun.html'>fun</a>, <a href='tag_networking.html'>networking</a></p>
<!-- text end -->
]]></description><link>https://shumiloff.ru/pro-profdeformaciyu.html</link>
<guid>https://shumiloff.ru/./pro-profdeformaciyu.html</guid>
<dc:creator>Evgeniy Shumilov</dc:creator>
<pubDate>Sat, 12 Jun 2021 15:22:56 +0500</pubDate></item>
</channel></rss>
